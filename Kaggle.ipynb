{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNyG6i3BuPoQ",
        "outputId": "94167a36-0a11-4695-c12b-ea33e2ab12b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "204/204 [==============================] - 16s 56ms/step - loss: 787.1642 - val_loss: 21.7934\n",
            "Epoch 2/20\n",
            "204/204 [==============================] - 10s 49ms/step - loss: 104.2075 - val_loss: 22.0868\n",
            "Epoch 3/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 101.1197 - val_loss: 22.9623\n",
            "Epoch 4/20\n",
            "204/204 [==============================] - 10s 49ms/step - loss: 97.5196 - val_loss: 22.7312\n",
            "Epoch 5/20\n",
            "204/204 [==============================] - 10s 51ms/step - loss: 95.8451 - val_loss: 22.8585\n",
            "Epoch 6/20\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 92.1561 - val_loss: 22.7421\n",
            "Epoch 7/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 88.8772 - val_loss: 21.0870\n",
            "Epoch 8/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 84.4153 - val_loss: 23.7799\n",
            "Epoch 9/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 77.7177 - val_loss: 18.8596\n",
            "Epoch 10/20\n",
            "204/204 [==============================] - 10s 49ms/step - loss: 73.3724 - val_loss: 20.3958\n",
            "Epoch 11/20\n",
            "204/204 [==============================] - 10s 48ms/step - loss: 65.5929 - val_loss: 19.6237\n",
            "Epoch 12/20\n",
            "204/204 [==============================] - 11s 52ms/step - loss: 60.1680 - val_loss: 20.9685\n",
            "Epoch 13/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 54.2711 - val_loss: 19.5908\n",
            "Epoch 14/20\n",
            "204/204 [==============================] - 10s 51ms/step - loss: 48.8477 - val_loss: 19.0478\n",
            "Epoch 15/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 41.8443 - val_loss: 19.5442\n",
            "Epoch 16/20\n",
            "204/204 [==============================] - 10s 49ms/step - loss: 37.3984 - val_loss: 18.6907\n",
            "Epoch 17/20\n",
            "204/204 [==============================] - 10s 47ms/step - loss: 31.6810 - val_loss: 18.6393\n",
            "Epoch 18/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 28.8366 - val_loss: 20.0431\n",
            "Epoch 19/20\n",
            "204/204 [==============================] - 10s 49ms/step - loss: 25.9892 - val_loss: 18.6994\n",
            "Epoch 20/20\n",
            "204/204 [==============================] - 10s 50ms/step - loss: 23.5314 - val_loss: 18.7121\n",
            "61/61 [==============================] - 2s 19ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# train.csv and test.csv files are correctly formatted and located\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X_train = train_data[\"sequence\"].values\n",
        "y_train = train_data[\"target\"].values\n",
        "X_test = test_data[\"sequence\"].values\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(X_train)  # Fit on training data\n",
        "\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = 500  # this was adjusted and caused the most amount of change in prediction score\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_length),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        LSTM(32),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Directly using the model for training and prediction to avoid complications\n",
        "model = create_model()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_padded, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Generating the predictions\n",
        "test_predictions = model.predict(X_test_padded)\n",
        "\n",
        "# Creating submission DataFrame\n",
        "submission_df = pd.DataFrame({'id': test_data['id'], 'target': test_predictions.flatten()})\n",
        "submission_df.to_csv('prediction.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview\n",
        "This project utilizes a Long Short-Term Memory (LSTM) model to predict sequence-based outcomes. The model is implemented using TensorFlow and Keras libraries and focuses on handling sequence data for prediction tasks.\n",
        "\n",
        "Requirements\n",
        "Python 3.8+\n",
        "pandas\n",
        "numpy\n",
        "scikit-learn\n",
        "TensorFlow 2.x\n",
        "Installation\n",
        "To set up the necessary environment:\n",
        "\n",
        "Install Python 3.8 or newer.\n",
        "Install the required Python packages using pip:\n",
        "Copy code\n",
        "pip install pandas numpy scikit-learn tensorflow\n",
        "Dataset\n",
        "Ensure you have the train.csv and test.csv files in the same directory as the script. These files should be properly formatted CSVs where:\n",
        "\n",
        "train.csv contains the columns sequence and target.\n",
        "test.csv contains the column sequence.\n",
        "Usage\n",
        "Run the script using the following command:\n",
        "\n",
        "Copy code\n",
        "python lstm_sequence_prediction.py\n",
        "This will train the model and output predictions into a file named prediction.csv in the same directory.\n",
        "\n",
        "Files\n",
        "lstm_sequence_prediction.py: Main Python script for the LSTM model.\n",
        "train.csv: Training data file.\n",
        "test.csv: Test data file for which predictions will be made.\n",
        "Model Details\n",
        "Model Architecture: Uses an embedding layer, followed by a bidirectional LSTM and dense layers.\n",
        "Training: Trained with a validation split of 20% for 20 epochs.\n",
        "Output: Predictions are saved in prediction.csv.\n"
      ],
      "metadata": {
        "id": "UAHA7Z3FWZ0y"
      }
    }
  ]
}